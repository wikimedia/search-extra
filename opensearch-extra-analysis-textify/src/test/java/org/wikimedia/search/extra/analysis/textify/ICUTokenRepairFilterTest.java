package org.wikimedia.search.extra.analysis.textify;

import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertNull;
import static org.wikimedia.search.extra.analysis.textify.ICUTokenRepairFilterTestUtils.testICUTokenization;
import static org.wikimedia.search.extra.analysis.textify.ICUTokenRepairFilterTestUtils.makeICUTokStream;

import java.io.IOException;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.BaseTokenStreamTestCase;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.core.WhitespaceTokenizer;
import org.apache.lucene.analysis.icu.tokenattributes.ScriptAttribute;
import org.junit.Test;

public class ICUTokenRepairFilterTest extends BaseTokenStreamTestCase {

    ICUTokenRepairFilterConfig cfg;

    @Test
    public void testUAX29Example() throws IOException {
        // UAX #29 example: Do not break within sequences of digits, or digits adjacent
        // to letters (â€œ3aâ€, or â€œA3â€).
        testICUTokenization("Ğ” 3a Ğ” A3", // input
            new String[]{"Ğ”", "3", "a", "Ğ”", "A3"}, // default tokens
            new String[]{"Ğ”", "3a", "Ğ”", "A3"},    // repaired tokens
            new String[]{"Cyrillic", "Latin", "Cyrillic", "Latin"}, // scripts
            new String[]{"<ALPHANUM>"}, // types - all ALPHANUM
            new int[]{0, 2, 5, 7}, // start offsets
            new int[]{1, 4, 6, 9}, // end offsets
            new int[]{1, 1, 1, 1}  // pos increments
        );
    }

    @Test
    public void testHomoglyphExamples() throws IOException {
        // Cyrillic Ğ¾ in choc*Ğ¾*late, Cyrillic Ğœ in Ğœoscow
        testICUTokenization("chocĞ¾late Ğœoscow", // input
            new String[]{"choc", "Ğ¾", "late", "Ğœ", "oscow"}, // default tokens
            new String[]{"chocĞ¾late", "Ğœoscow"}, // repaired tokens
            new String[]{"Unknown"},     // scripts - all Unknown
            new String[]{"<ALPHANUM>"}, // types - all ALPHANUM
            new int[]{0, 10},          // start offsets
            new int[]{9, 16},         // end offsets
            new int[]{1,  1}         // pos increments
        );
    }

    @Test
    public void testBasicICUTokenRepair() throws IOException {
        // Latin/Cyrillic/Greek ABC; plus intentional examples from enwiki
        testICUTokenization("abcĞ°Ğ±Ğ³Î±Î²Î³ SWÎ›NKĞ£ lÎ¹Ğ¼Î¹Ñ‚ed edÎ¹Ñ‚Î¹on", // input
            new String[]{"abc", "Ğ°Ğ±Ğ³", "Î±Î²Î³", "SW", "Î›", "NK", "Ğ£",
                "l", "Î¹", "Ğ¼", "Î¹", "Ñ‚", "ed", "ed", "Î¹", "Ñ‚", "Î¹", "on"}, // default tokens
            new String[]{"abcĞ°Ğ±Ğ³Î±Î²Î³", "SWÎ›NKĞ£", "lÎ¹Ğ¼Î¹Ñ‚ed", "edÎ¹Ñ‚Î¹on"},    // repaired tokens
            new String[]{"Unknown"},     // scripts - all Unknown
            new String[]{"<ALPHANUM>"}, // types - all ALPHANUM
            new int[]{0, 10, 17, 25},  // start offsets
            new int[]{9, 16, 24, 32}, // end offsets
            new int[]{1,  1,  1,  1} // pos increments
        );
    }

    @Test
    public void testNumberSplits() throws IOException {
        // earlier character sets cause later splits after whitespace!
        testICUTokenization("3Q 3Î© 3Î© 3Ğ” 3Ğ” 3Q", // input
            new String[]{"3Q", "3", "Î©", "3Î©", "3", "Ğ”", "3Ğ”", "3", "Q"}, // default tokens
            new String[]{"3Q", "3Î©", "3Î©", "3Ğ”", "3Ğ”", "3Q"}, // repaired tokens
            new String[]{"Latin", "Greek", "Greek", "Cyrillic",
                "Cyrillic", "Latin"},       // scripts
            new String[]{"<ALPHANUM>"},     // types - all ALPHANUM
            new int[]{0, 3, 6,  9, 12, 15}, // start offsets
            new int[]{2, 5, 8, 11, 14, 17}, // end offsets
            new int[]{1, 1, 1,  1,  1,  1}  // pos increments
        );

        testICUTokenization("3Q3 3Î©3",         // input
            new String[]{"3Q3", "3", "Î©3"},   // default tokens
            new String[]{"3Q3", "3Î©3"},      // repaired tokens
            new String[]{"Latin", "Greek"}, // scripts
            new String[]{"<ALPHANUM>"},    // types - all ALPHANUM
            new int[]{0, 4},              // start offsets
            new int[]{3, 7},             // end offsets
            new int[]{1, 1}             // pos increments
        );

        // longer distance example
        testICUTokenization("3Q 1234567890 1234567890 3Î©",              // input
            new String[]{"3Q", "1234567890", "1234567890", "3", "Î©"},   // default tokens
            new String[]{"3Q", "1234567890", "1234567890", "3Î©"},       // repaired tokens
            new String[]{"Latin", "Common", "Common", "Greek"},         // scripts
            new String[]{"<ALPHANUM>", "<NUM>", "<NUM>", "<ALPHANUM>"}, // types
            new int[]{0,  3, 14, 25},                                   // start offsets
            new int[]{2, 13, 24, 27},                                   // end offsets
            new int[]{1,  1,  1,  1}                                    // pos increments
        );

        // digit 2 in many scripts
        testICUTokenization("à¥¨à§¨à©¨à«¨á ’á¥ˆß‚á§’á­’",                               // input
            new String[]{"à¥¨", "à§¨", "à©¨", "à«¨", "á ’", "á¥ˆ", "ß‚", "á§’", "á­’"}, // default tokens
            new String[]{"à¥¨à§¨à©¨à«¨á ’á¥ˆß‚á§’á­’"},                                 // repaired tokens
            new String[]{"Common"},                                   // scripts
            new String[]{"<NUM>"}                                     // types
        );

        // "123" is <NUM> and should be rejoined with á€
        testICUTokenization("xx 123á€",         // input
            new String[]{"xx", "123", "á€"},   // default tokens
            new String[]{"xx", "123á€"},      // repaired tokens
            new String[]{"Latin", "Khmer"}, // scripts
            new String[]{"<ALPHANUM>"}     // types
        );

        // "xx123" is not <NUM>, should be repaired to <ALPHANUM>, and blocked from joining á€
        testICUTokenization("xx123á€",          // input
            new String[]{"xx123", "á€"},       // default tokens
            new String[]{"xx123", "á€"},      // repaired tokens
            new String[]{"Latin", "Khmer"}, // scripts
            new String[]{"<ALPHANUM>"}     // types
        );
    }

    @Test
    public void testMegaMultiScriptExample() throws IOException {
        String[] multiScriptChars = {
            // Latin, Greek, Mandaic, Samaritan, N'Ko, Thaana, Arabic, Hebrew,
            "d", "Ï—", "à¡ƒ", "à „", "ß„", "Ş”", "Ø¦", "×—",
            // Cyrillic, Armenian, Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil,
            "Ğ´", "Õ¤", "à¤„", "à¦…", "à¨–", "àª–", "à¬•", "à®´",
            // Telugu, Kannada, Malayalam, Sinhala, Thai, Lao, Myanmar, Georgian,
            "à°¨", "à²¤", "à´•", "à¶•", "à¸“", "àº—", "á€–", "â´”",
            // Ethiopic,, Cherokee, Canadian Syllabics, Ogham, Runic
            "á‰", "ê­³", "á•", "áš…", "áš¥"
        };

        String megaToken = String.join("", multiScriptChars); // all one token
        cfg = new ICUTokenRepairFilterConfig();
        cfg.setNoScriptLimits();
        testICUTokenization(megaToken, cfg, // input & config
            multiScriptChars, // default tokens â€”Â all separated!
            new String[]{megaToken}, // repaired tokens - all together!
            new String[]{"Unknown"}, // scripts
            new String[]{"<ALPHANUM>"} // types
        );
    }

    @Test
    public void testSurrogateCamelCase() throws IOException {
        // make sure 32-bit uppercase and lowercase are recognized as numbers

        // ğ€ğ©ğª is Deseret (U+10400-U+1044F), which is the only alphabet with 32-bit
        // characters that has upper and lowercase letters recognized by our current
        // version of Java (8), and which the current ICU tokenizer (8.7) does not mark
        // as "Common" script

        String surrogateCamel = "ğ€ğ©ğªAbc Abcğ€ğ©ğª ğ¨ğ©ğªabc abcğ¨ğ©ğª";

        // defaults - camelCase should *not* be rejoined
        cfg = new ICUTokenRepairFilterConfig();
        cfg.setNoScriptLimits();
        testICUTokenization(surrogateCamel, cfg, // input & config
            new String[]{"ğ€ğ©ğª", "Abc", "Abc", "ğ€ğ©ğª", "ğ¨ğ©ğª", "abc", "abc", "ğ¨ğ©ğª"}, // default tokens
            new String[]{"ğ€ğ©ğª", "Abc", "Abc", "ğ€ğ©ğª", "ğ¨ğ©ğªabc", "abcğ¨ğ©ğª"}, // repaired tokens
            new String[]{"Deseret", "Latin", "Latin", "Deseret", "Unknown", "Unknown"}, // scripts
            new String[]{"<ALPHANUM>"} // types
        );

        // don't preserve camelCase splits
        cfg.setKeepCamelSplit(false);
        testICUTokenization(surrogateCamel, cfg, // input & config
            new String[]{"ğ€ğ©ğªAbc", "Abcğ€ğ©ğª", "ğ¨ğ©ğªabc", "abcğ¨ğ©ğª"}, // repaired tokens
            new String[]{"Unknown"}, // scripts
            new String[]{"<ALPHANUM>"} // types
        );
    }

    @Test
    public void testSurrogateDigits() throws IOException {
        // make sure 32-bit numbers are recognized as numbers

        // Latin a + Takri 3 (U+116C3) + Brahmi 3 (U+11069) + Latin z
        // These digits are carefully chosen to be 32-bit and recognized by our current
        // version of Java (8) as having a type of DECIMAL_DIGIT_NUMBER
        String surrogateDigits = "ağ‘›ƒğ‘©z";

        cfg = new ICUTokenRepairFilterConfig();
        cfg.setNoScriptLimits();

        // defaults
        testICUTokenization(surrogateDigits, cfg, // input & config
            new String[]{"a", "ğ‘›ƒ", "ğ‘©", "z"}, // default tokens
            new String[]{surrogateDigits}, // repaired token
            new String[]{"Unknown"}, // scripts
            new String[]{"<ALPHANUM>"} // types
        );

        // only allow number splits
        cfg.setMergeNumOnly(true);
        testICUTokenization(surrogateDigits, cfg, // input & config
            new String[]{surrogateDigits}, // repaired tokens"
            new String[]{"Unknown"}, // scripts
            new String[]{"<ALPHANUM>"} // types
        );
    }

    /* Monoscript tokens in scripts that don't need segmentation should be unchanged. Mini-test
     * of the ICU tokenizer's script identification; Baseline test of testICUTokenization().
     */
    @Test
    public void testMiscMonoscriptTokens() throws IOException {
        // take the tokens, join them together, send them off to be split apart
        // and get the same tokens back, ICU repair or not
        String[] toks = {"Wikipedia", "Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ñ", "Î’Î¹ÎºÎ¹Ï€Î±Î¯Î´ÎµÎ¹Î±", "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡"};
        testICUTokenization(String.join(" ", toks), // input
            toks, // default tokens
            toks, // repaired tokens
            new String[]{"Latin", "Cyrillic", "Greek", "Armenian"}, // scripts
            new String[]{"<ALPHANUM>"} // types - all ALPHANUM
        );

        // That was fun! Do it again!
        toks = new String[]{"áƒ•áƒ˜áƒ™áƒ˜áƒáƒ”áƒ“áƒ˜áƒ", "à¤µà¤¿à¤•à¤¿à¤ªà¥€à¤¡à¤¿à¤¯à¤¾", "à®µà®¿à®•à¯à®•à®¿à®ªà¯à®ªà¯€à®Ÿà®¿à®¯à®¾", "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾"};
        testICUTokenization(String.join(" ", toks), toks, toks,
            new String[]{"Georgian", "Devanagari", "Tamil", "Bengali"},
            new String[]{"<ALPHANUM>"} // types - all ALPHANUM
        );
    }

    /* We should avoid adding script attributes (with default values) to token streams
     * that don't already have them (i.e., because a non-ICU tokenizer was used).
     */
    @Test
    public void testAvoidAddingScriptAttributes() throws IOException {
        // Cyrillic Ğ¾ in chocĞ¾late, Cyrillic Ğœ in Ğœoscow
        String testInput = "chocĞ¾late Ğœoscow SWÎ›NKĞ£ lÎ¹Ğ¼Î¹Ñ‚ed edÎ¹Ñ‚Î¹on NGiĞ˜X KoĞ¯n";

        // ICU tokenizer -> ScriptAttribute *is not* null
        TokenStream ts = makeICUTokStream(testInput);
        ScriptAttribute scriptAtt = ts.getAttribute(ScriptAttribute.class);
        assertNotNull(scriptAtt);

        // Non-ICU tokenizer -> ScriptAttribute *is* null, even with repair
        Analyzer ana = new Analyzer() {
            @Override
            protected TokenStreamComponents createComponents(String fieldName) {
                Tokenizer tok = new WhitespaceTokenizer(); // <= doesn't need repair!
                TokenStream ts = new ICUTokenRepairFilter(tok);
                return new TokenStreamComponents(tok, ts);
            }
        };
        ts = ana.tokenStream("", testInput);
        scriptAtt = ts.getAttribute(ScriptAttribute.class);
        assertNull(scriptAtt);
    }

}
